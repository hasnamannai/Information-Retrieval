{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LlGJc-0P1Nwd","executionInfo":{"status":"ok","timestamp":1700346983409,"user_tz":-60,"elapsed":8667,"user":{"displayName":"Hasna Mannai","userId":"10235284771390570471"}},"outputId":"80edd5e6-0716-4207-86a9-926e0e235ba3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rank_bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n","Installing collected packages: rank_bm25\n","Successfully installed rank_bm25-0.2.2\n","Cloning into 'project1-2023'...\n","remote: Enumerating objects: 8, done.\u001b[K\n","remote: Counting objects: 100% (8/8), done.\u001b[K\n","remote: Compressing objects: 100% (6/6), done.\u001b[K\n","remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (8/8), 2.30 MiB | 5.22 MiB/s, done.\n"]}],"source":["!pip install rank_bm25\n","!git clone https://github.com/cr-nlp/project1-2023.git"]},{"cell_type":"markdown","source":["Reranking with Word2Vect and changing bm25 parameters"],"metadata":{"id":"csDHG4kDtuSL"}},{"cell_type":"code","source":["from rank_bm25 import BM25Okapi\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","from sklearn.metrics import ndcg_score\n","from collections import defaultdict\n","\n","# Download NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Step 1: Load NFCorpus Dataset\n","def loadNFCorpus():\n","    dir = \"./project1-2023/\"\n","    filename_doc = dir + \"dev.docs\"\n","    filename_queries = dir + \"dev.all.queries\"\n","    filename_qrel = dir + \"dev.2-1-0.qrel\"\n","\n","    dicDoc = dict()\n","    with open(filename_doc) as file:\n","        for line in file:\n","            key, value = line.split('\\t')\n","            dicDoc[key] = value\n","\n","    dicReq = dict()\n","    with open(filename_queries) as file:\n","        for line in file:\n","            key, value = line.split('\\t')\n","            dicReq[key] = value\n","\n","    dicReqDoc = defaultdict(dict)\n","    with open(filename_qrel) as file:\n","        for line in file:\n","            req, _, doc, score = line.strip().split('\\t')\n","            dicReqDoc[req][doc] = int(score)\n","\n","    return dicDoc, dicReq, dicReqDoc\n","\n","# Function to preprocess text\n","def preprocess_text(text):\n","    stop_words = set(stopwords.words('english'))\n","    tokens = word_tokenize(text.lower())\n","    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n","    return tokens\n","\n","from gensim.models import Word2Vec\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def train_word2vec_model(corpus):\n","    model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n","    return model\n","\n","def get_vector(word2vec_model, word):\n","    return word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(word2vec_model.vector_size)\n","\n","def document_vector(word2vec_model, doc_tokens):\n","    return np.mean([get_vector(word2vec_model, word) for word in doc_tokens], axis=0)\n","\n","\n","# Function to run BM25 retrieval\n","def run_bm25(dicDoc, dicReq, dicReqDoc, startDoc, endDoc, bm25_k1=3, bm25_b=0.80):\n","    docsToKeep = []\n","    reqsToKeep = []\n","    dicReqDocToKeep = defaultdict(dict)\n","\n","    i = startDoc\n","    for reqId in dicReqDoc:\n","        if i > (endDoc - startDoc):\n","            break\n","        for docId in dicReqDoc[reqId]:\n","            dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n","            docsToKeep.append(docId)\n","            i = i + 1\n","        reqsToKeep.append(reqId)\n","    docsToKeep = list(set(docsToKeep))\n","\n","    # Preprocess documents\n","    allVocab = set()\n","    for k in docsToKeep:\n","        docTokenList = preprocess_text(dicDoc[k])\n","        allVocab.update(docTokenList)\n","    allVocabListDoc = list(allVocab)\n","\n","    # Preprocess queries\n","    allVocab = set()\n","    for k in reqsToKeep:\n","        reqTokenList = preprocess_text(dicReq[k])\n","        allVocab.update(reqTokenList)\n","    allVocabListReq = list(allVocab)\n","\n","    # BM25 indexing\n","    corpusDocTokenList = [preprocess_text(dicDoc[k]) for k in docsToKeep]\n","    bm25 = BM25Okapi(corpusDocTokenList, k1=bm25_k1, b=bm25_b)\n","\n","    ndcgBM25Cumul = 0\n","    nbReq = 0\n","\n","\n","    # Run BM25 and then rerank with Word2Vec\n","    ndcgRerankCumul = 0\n","    for req in reqsToKeep:\n","        reqTokenList = preprocess_text(dicReq[req])\n","        doc_scores = bm25.get_scores(reqTokenList)\n","        top_doc_indices = np.argsort(doc_scores)[::-1][:5]  # Get top 5 documents\n","\n","        query_vector = document_vector(word2vec_model, reqTokenList)\n","        rerank_scores = []\n","        trueDocs = []\n","        for idx in top_doc_indices:\n","            docId = docsToKeep[idx]\n","            doc_vector = document_vector(word2vec_model, corpusDocTokenList[idx])\n","            rerank_score = cosine_similarity([query_vector], [doc_vector])[0][0]\n","            rerank_scores.append(rerank_score)\n","\n","            # Error handling for missing document IDs\n","            true_score = dicReqDocToKeep[req].get(docId, 0)\n","            trueDocs.append(true_score)\n","\n","        ndcgRerankCumul += ndcg_score([trueDocs], [rerank_scores])\n","\n","    ndcgRerankCumul /= len(reqsToKeep)\n","    print(\"Average NDCG after reranking with Word2Vec:\", ndcgRerankCumul)\n","    return ndcgBM25Cumul\n","\n","# Run BM25 retrieval using NFCorpus data\n","dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n","# Train Word2Vec model\n","corpus = [preprocess_text(doc) for doc in dicDoc.values()]\n","word2vec_model = train_word2vec_model(corpus)\n","\n","nb_docs = 3192  # all docs\n","run_bm25(dicDoc, dicReq, dicReqDoc, 0, nb_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9IcEFVbjZ6S","executionInfo":{"status":"ok","timestamp":1700347057061,"user_tz":-60,"elapsed":51592,"user":{"displayName":"Hasna Mannai","userId":"10235284771390570471"}},"outputId":"ac9cd039-bd74-491f-fa2c-3b9b3e12a5f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Average NDCG after reranking with Word2Vec: 0.59248720026296\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Reranking with Word2Vec and adding lemmatization"],"metadata":{"id":"Vyhi7JMTur8R"}},{"cell_type":"code","source":["from rank_bm25 import BM25Okapi\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","from sklearn.metrics import ndcg_score\n","from collections import defaultdict\n","\n","# Download NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Step 1: Load NFCorpus Dataset\n","def loadNFCorpus():\n","    dir = \"./project1-2023/\"\n","    filename_doc = dir + \"dev.docs\"\n","    filename_queries = dir + \"dev.all.queries\"\n","    filename_qrel = dir + \"dev.2-1-0.qrel\"\n","\n","    dicDoc = dict()\n","    with open(filename_doc) as file:\n","        for line in file:\n","            key, value = line.split('\\t')\n","            dicDoc[key] = value\n","\n","    dicReq = dict()\n","    with open(filename_queries) as file:\n","        for line in file:\n","            key, value = line.split('\\t')\n","            dicReq[key] = value\n","\n","    dicReqDoc = defaultdict(dict)\n","    with open(filename_qrel) as file:\n","        for line in file:\n","            req, _, doc, score = line.strip().split('\\t')\n","            dicReqDoc[req][doc] = int(score)\n","\n","    return dicDoc, dicReq, dicReqDoc\n","\n","# Function to preprocess text\n","def preprocess_text(text):\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Remove punctuation and convert to lowercase\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text.lower())\n","\n","    # Tokenization using NLTK\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords and apply lemmatization\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n","\n","    return tokens\n","\n","from gensim.models import Word2Vec\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def train_word2vec_model(corpus):\n","    model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n","    return model\n","\n","def get_vector(word2vec_model, word):\n","    return word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(word2vec_model.vector_size)\n","\n","def document_vector(word2vec_model, doc_tokens):\n","    return np.mean([get_vector(word2vec_model, word) for word in doc_tokens], axis=0)\n","\n","\n","# Function to run BM25 retrieval\n","def run_bm25(dicDoc, dicReq, dicReqDoc, startDoc, endDoc, bm25_k1=3, bm25_b=0.80):\n","    docsToKeep = []\n","    reqsToKeep = []\n","    dicReqDocToKeep = defaultdict(dict)\n","\n","    i = startDoc\n","    for reqId in dicReqDoc:\n","        if i > (endDoc - startDoc):\n","            break\n","        for docId in dicReqDoc[reqId]:\n","            dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n","            docsToKeep.append(docId)\n","            i = i + 1\n","        reqsToKeep.append(reqId)\n","    docsToKeep = list(set(docsToKeep))\n","\n","    # Preprocess documents\n","    allVocab = set()\n","    for k in docsToKeep:\n","        docTokenList = preprocess_text(dicDoc[k])\n","        allVocab.update(docTokenList)\n","    allVocabListDoc = list(allVocab)\n","\n","    # Preprocess queries\n","    allVocab = set()\n","    for k in reqsToKeep:\n","        reqTokenList = preprocess_text(dicReq[k])\n","        allVocab.update(reqTokenList)\n","    allVocabListReq = list(allVocab)\n","\n","    # BM25 indexing\n","    corpusDocTokenList = [preprocess_text(dicDoc[k]) for k in docsToKeep]\n","    bm25 = BM25Okapi(corpusDocTokenList, k1=bm25_k1, b=bm25_b)\n","\n","    ndcgBM25Cumul = 0\n","    nbReq = 0\n","\n","\n","    # Run BM25 and then rerank with Word2Vec\n","    ndcgRerankCumul = 0\n","    for req in reqsToKeep:\n","        reqTokenList = preprocess_text(dicReq[req])\n","        doc_scores = bm25.get_scores(reqTokenList)\n","        top_doc_indices = np.argsort(doc_scores)[::-1][:5]  # Get top 5 documents\n","\n","        query_vector = document_vector(word2vec_model, reqTokenList)\n","        rerank_scores = []\n","        trueDocs = []\n","        for idx in top_doc_indices:\n","            docId = docsToKeep[idx]\n","            doc_vector = document_vector(word2vec_model, corpusDocTokenList[idx])\n","            rerank_score = cosine_similarity([query_vector], [doc_vector])[0][0]\n","            rerank_scores.append(rerank_score)\n","\n","            # Error handling for missing document IDs\n","            true_score = dicReqDocToKeep[req].get(docId, 0)\n","            trueDocs.append(true_score)\n","\n","        ndcgRerankCumul += ndcg_score([trueDocs], [rerank_scores])\n","\n","    ndcgRerankCumul /= len(reqsToKeep)\n","    print(\"Average NDCG after reranking with Word2Vec:\", ndcgRerankCumul)\n","    return ndcgBM25Cumul\n","\n","# Run BM25 retrieval using NFCorpus data\n","dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n","# Train Word2Vec model\n","corpus = [preprocess_text(doc) for doc in dicDoc.values()]\n","word2vec_model = train_word2vec_model(corpus)\n","\n","nb_docs = 3192  # all docs\n","run_bm25(dicDoc, dicReq, dicReqDoc, 0, nb_docs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4mfsD3pfuDMH","executionInfo":{"status":"ok","timestamp":1700349482924,"user_tz":-60,"elapsed":57705,"user":{"displayName":"Hasna Mannai","userId":"10235284771390570471"}},"outputId":"2210b5e7-15b4-434c-dcdc-44383e8ad7f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Average NDCG after reranking with Word2Vec: 0.5982509347285934\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Reranking with BERT"],"metadata":{"id":"5n9nLQNmt0-F"}},{"cell_type":"code","source":["!pip install sentence-transformers\n"],"metadata":{"id":"1ezEQfe1nVBd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700347227655,"user_tz":-60,"elapsed":10673,"user":{"displayName":"Hasna Mannai","userId":"10235284771390570471"}},"outputId":"0915a113-b0c9-46cc-fd9b-68c99201fd6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/86.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Collecting sentencepiece (from sentence-transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=43ab44af100bb02571400880bf7ff6e4777c70bf772fa9a91e18d1f525783ec9\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence-transformers\n","Installing collected packages: sentencepiece, sentence-transformers\n","Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTFbjMAXrM9D","executionInfo":{"status":"ok","timestamp":1700347234377,"user_tz":-60,"elapsed":229,"user":{"displayName":"Hasna Mannai","userId":"10235284771390570471"}},"outputId":"090c53f7-0cc0-4474-9428-0b5b8f0f74e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from rank_bm25 import BM25Okapi\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import string\n","import numpy as np\n","from sklearn.metrics import ndcg_score\n","from collections import defaultdict\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Download NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Download pre-trained BERT model\n","model_name = 'paraphrase-MiniLM-L6-v2'\n","bert_model = SentenceTransformer(model_name)\n","\n","\n","def loadNFCorpus():\n","    dir = \"./project1-2023/\"\n","    filename_doc = dir + \"dev.docs\"\n","    filename_queries = dir + \"dev.all.queries\"\n","    filename_qrel = dir + \"dev.2-1-0.qrel\"\n","\n","    dicDoc = dict()\n","    with open(filename_doc) as file:\n","        for line in file:\n","            key, value = line.split('\\t')\n","            dicDoc[key] = value\n","\n","    dicReq = dict()\n","    with open(filename_queries) as file:\n","        for line in file:\n","            key, value = line.split('\\t')\n","            dicReq[key] = value\n","\n","    dicReqDoc = defaultdict(dict)\n","    with open(filename_qrel) as file:\n","        for line in file:\n","            req, _, doc, score = line.strip().split('\\t')\n","            dicReqDoc[req][doc] = int(score)\n","\n","    return dicDoc, dicReq, dicReqDoc\n","\n","# Instantiate WordNetLemmatizer outside the function\n","lemmatizer = WordNetLemmatizer()\n","\n","# Function to preprocess text\n","def preprocess_text(text):\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Remove punctuation and convert to lowercase\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text.lower())\n","\n","    # Tokenization using NLTK\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords and apply lemmatization\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n","\n","    return tokens\n","\n","# Function to get BERT embeddings\n","def get_bert_embedding(model, text):\n","    return model.encode(text)\n","\n","# Function to run BM25 retrieval with BERT reranking\n","def run_bm25_with_bert(dicDoc, dicReq, dicReqDoc, startDoc, endDoc, bm25_k1=1.5, bm25_b=0.75):\n","    docsToKeep = []\n","    reqsToKeep = []\n","    dicReqDocToKeep = defaultdict(dict)\n","\n","    doc_count = startDoc\n","    for reqId in dicReqDoc:\n","        if doc_count > (endDoc - startDoc):\n","            break\n","        for docId in dicReqDoc[reqId]:\n","            dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n","            docsToKeep.append(docId)\n","            doc_count += 1\n","        reqsToKeep.append(reqId)\n","    docsToKeep = list(set(docsToKeep))\n","\n","    # Preprocess documents\n","    allVocab = set()\n","    for k in docsToKeep:\n","        docTokenList = preprocess_text(dicDoc[k])\n","        allVocab.update(docTokenList)\n","    allVocabListDoc = list(allVocab)\n","\n","    # Preprocess queries\n","    allVocab = set()\n","    for k in reqsToKeep:\n","        reqTokenList = preprocess_text(dicReq[k])\n","        allVocab.update(reqTokenList)\n","    allVocabListReq = list(allVocab)\n","\n","    # BM25 indexing\n","    corpusDocTokenList = np.array([preprocess_text(dicDoc[k]) for k in docsToKeep])\n","    bm25 = BM25Okapi(corpusDocTokenList, k1=bm25_k1, b=bm25_b)\n","\n","    ndcgBM25Cumul = 0\n","    nbReq = 0\n","\n","    # Run BM25 and then rerank with BERT\n","    ndcgRerankCumul = 0\n","    for req in reqsToKeep:\n","        reqTokenList = preprocess_text(dicReq[req])\n","\n","        # Get BERT embeddings for the query\n","        query_embedding = get_bert_embedding(bert_model, ' '.join(reqTokenList))\n","\n","        # Get BM25 scores\n","        doc_scores = bm25.get_scores(reqTokenList)\n","        top_doc_indices = np.argsort(doc_scores)[::-1][:5]  # Get top 5 documents\n","\n","        rerank_scores = []\n","        trueDocs = []\n","        for idx in top_doc_indices:\n","            docId = docsToKeep[idx]\n","\n","            # Get BERT embeddings for the document\n","            doc_embedding = get_bert_embedding(bert_model, dicDoc[docId])\n","\n","            # Calculate cosine similarity between query and document embeddings\n","            rerank_score = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n","            rerank_scores.append(rerank_score)\n","\n","            # Error handling for missing document IDs\n","            true_score = dicReqDocToKeep[req].get(docId, 0)\n","            trueDocs.append(true_score)\n","\n","        ndcgRerankCumul += ndcg_score([trueDocs], [rerank_scores])\n","\n","    ndcgRerankCumul /= len(reqsToKeep)\n","    print(\"Average NDCG after reranking with BERT:\", ndcgRerankCumul)\n","    return ndcgBM25Cumul\n","\n","# Run BM25 retrieval using NFCorpus data\n","dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n","\n","# Specify the number of documents to process\n","nb_docs = 3192  # all docs\n","\n","# Run BM25 retrieval with BERT reranking\n","run_bm25_with_bert(dicDoc, dicReq, dicReqDoc, 0, nb_docs)\n"],"metadata":{"id":"ehyqWOpenU_a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700332170645,"user_tz":-60,"elapsed":71718,"user":{"displayName":"Hasna Mannai","userId":"10235284771390570471"}},"outputId":"3f2cd558-0691-4445-b365-483b635df359"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","<ipython-input-6-9fa25ce7cc6b>:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  corpusDocTokenList = np.array([preprocess_text(dicDoc[k]) for k in docsToKeep])\n"]},{"output_type":"stream","name":"stdout","text":["Average NDCG after reranking with BERT: 0.675554045841826\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"uUZFlNQLnU58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700407855259,"user_tz":-60,"elapsed":26786,"user":{"displayName":"Hasna Mannai","userId":"10235284771390570471"}},"outputId":"f504274f-0724-4014-d160-1bdee410cb26"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RavkqipjcZxt"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}